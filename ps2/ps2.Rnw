%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=2.54cm,bmargin=2.54cm,lmargin=3cm,rmargin=3cm}
\begin{document}


\title{STAT243 Problem Set2}


\author{Name: Chih Hui Wang SID: 26955255}


\date{September 5, 2015}

\maketitle
<<setup, include=FALSE>>=
knitr::opts_chunk$set(fig.align='center')
knitr::opts_chunk$set(comment="") 
knitr::opts_chunk$set(fig.width=5, fig.height=5)   
@

1. (a) I will use R and bash to deal with the problem. Use bash to
make the size of file small and R to do random sampling. Firt, I use
\textbf{curl} to download the bz2 file. and rename it as \textbf{PS2data.bz2}.
At this point, we are not allowd to directly unzip the data, so I
use the modifier \textbf{-c} to output the content to observe data.
It will not directly unzip the file.

<<download, engine='bash', eval=FALSE>>=
#Download
curl -s "www.stat.berkeley.edu/share/paciorek/ss13hus.csv.bz2" -o PS2data.bz2
@

<<show, engine='bash'>>=
#Show data
bunzip2 -c PS2data.bz2 | head -n2
@

Now we turn to R to do some processing. Change my R working directory
to \textbf{Work}, and use \textbf{list.files()} to check whether I
am in the right directory.

<<changedir>>=
setwd("C:/Users/jason/Desktop/Work")
#To check we are in the correct working directory 
list.files()
@

Because we only need certain column, I use \textbf{read.csv} to get
the first row and later use R to process it. To avoid the string setting
as factor, I use \textbf{stringsAsFactors=FALSE}. 

<<allcolumn>>=
allcol <- read.csv("PS2data.bz2", nrow=1, header=FALSE, stringsAsFactors=FALSE) 
@

To see where those columns locate, I use \textbf{\%in\%}, which give
us a row of TRUE/FALSE,and \textbf{which} , which give me the location,to
get which columns are those we want. After I use paste with argument
collapse to make the vector into a string, output the result to number.txt
and I can use cut in bash to retrieve those row to decrease the file
size.

<<colname>>=
#Column we want 
col_want <- c("ST", "NP", "BDSP", "BLD", "RMSP", "TEN", "FINCP",
			  "FPARC", "HHL", "NOC", "MV", "VEH", "YBL")
 
#Find out which column we want to keep
coln_want <- which(allcol %in% col_want)
#Output the result for bash
write.table(paste(coln_want, collapse=","), "number.txt", row.names=FALSE, col.names=FALSE) 
@

I store the column number in variable \textbf{number}. I use sed to
remove `` by nothing because when using the modifier \textbf{-f}
for \textbf{cut}, we use \textbf{,} as seperator for column. For example,
to pull out one and second columns $\rightarrow$ cut -d',' -f1,2
filename.

After we got those numbers for columns we want, we can use cut to
take out those column and pipe it into command \textbf{gzip} to zip
the result, named PS2data.gz. 

<<columnprocess, engine='bash'>>=
#Get the colname we want
col_n=$(cat number.txt)
#Get all the column number
number=$(echo $col_n | sed 's/"//g') 
echo $number
#Eliminate other columns and zip the result
bunzip2 -c PS2data.bz2 | cut -d',' -f$number | gzip -c > PS2data.gz
@

Before I start to write my function, I need to know how many rows
in the data which then I can sample the row Index from it. I use \textbf{readLines}
and file connection here. It will read 100000 row for each time until
there are no data anymore by \textbf{while} loop. I initialize l by
1 to start the while loop, so in the end I have to substract 2 from
\textbf{total\_row}, 1 for the l and 1 for the header.

<<linecount>>=
total_row <- 0 
datacon <- file("PS2data.gz", open="r") 
#Initial 
l <- 1 
while(l > 0){   
	total_row <- total_row + l   
	l <- length(readLines(datacon, n=100000)) 
} 
close(datacon); rm(datacon)
total_row <- total_row - 2
#Total number of row in the data
total_row
@

\textbf{my.read.csv} contain 3 arguments, file's name \textbf{(filename)},
number of sample \textbf{(n)}, number of total row \textbf{(total\_row)},
how many rows we want to read each time \textbf{(maxrow)}. I first
creat a variable, \textbf{breaks}, containing numbers from 1 to \Sexpr{format((total_row %/% 100000 + 1)*100000 + 1, scientific=FALSE)}
by every 100000 for later usage. The reason to use \Sexpr{format((total_row %/% 100000 + 1)*100000 + 1, scientific=FALSE)}
is that there are data from row 7200001 to 7219000. Then the function
will first create a all-0 data frame by n and the length of colname.
To avoid the mismatch name, it does not use the colname users provide;
instead, it reads the first line, gets its name and puts it to our
all-0 data frame. After sampling \textbf{(row\_want)}, it uses file
to open a reading connection, which can help us keep the pointer and
escape the slow speed by skipping large number. Then it runs a for-loop
which containing the process of reading 100000 each time (\textbf{read.csv}
with argument \textbf{nrow=maxrow}) and also finds the corresponding
row we want in our sample \textbf{(row\_want)} as well as updates
the row index \textbf{j}. Finally it closes the connection and renders
the sample data for user.

There are several points that I should clarify in my function. First,
the reason why I do not use the \textbf{col\_want}(previous variable)
as the name of the data is that when we compare the name by \textbf{\%in\%}
and \textbf{which}, the output will not give location exactly same
as the order of our col\_want. Hence, it may be safer to directly
use data name. Second, when I wirte the for loop, I use\textbf{ length(breaks)
- 1}. Because the last number of breaks is \Sexpr{format((total_row %/% 100000 + 1)*100000 + 1, scientific=FALSE)}
which is bigger than the length of our data, it should not go through
the operation in the for loop. Thrid, I use \textbf{file} function
to create a reading connection here. By doing so, I can avoid use
the \textbf{skip} function in \textbf{read.csv} which makes the speed
of reading the data in the last very slow; in other words, I have
to skip a large number of row.\newpage{}

<<function1>>=
#function 
my.read.csv <- function(filename, n, total_row,maxrow=100000){ 
		upper <- (total_row %/% maxrow) + 2   
		breaks <- seq(1, maxrow*upper + 1, by=maxrow)   
		datacon <- file(filename, open='r')   
		col_name <- read.csv(datacon, nrow=1, header=FALSE, stringsAsFactors=FALSE)   
		data <- matrix(0, nrow=n, ncol=length(col_name))   
		data <- as.data.frame(data)  
		#Avoid miss match   
		names(data) <- col_name   
		#Sample   
		n.row <- sample(1:total_row, n)     
		#Index   
		j <- 1   
		for(i in 1:(length(breaks) - 1)){     
			row_want <- which(n.row >= breaks[i] & n.row < breaks[i + 1])
			if(length(row_want) == 0){
				next     
			}else{       
			data[j:(j + length(row_want) - 1), ] <- read.csv(datacon, nrow=maxrow, header=FALSE)[n.row[row_want] - breaks[i] + 1, ]       
			j <- j + length(row_want)       
			}   
		}   
		close(datacon)   
		data 
} 
set.seed(1) 
newdata <- my.read.csv("PS2data.gz", n=10000, total_row, maxrow=100000) 
write.csv(newdata, "NewPS2data.csv", row.names=FALSE)
head(newdata)
@

To check that I have output the file correctly.

<<check, engine='bash'>>=
ls -lh NewPS2data.csv
cat NewPS2data.csv | head -n5
@

\newpage{}(b) In this problem, I will compare three way to read the
data and sample it, which is \textbf{read.csv}, \textbf{read\_csv},
\textbf{readLines}. My function's strutcure will basically be the
same as the one I write about (\textbf{my.read.csv}). I finished the
one for read.csv. 

Now, let us first deal with the function for \textbf{read\_csv} in
the package \textbf{\textit{readr}}. For \textbf{read\_csv}, the connection
will be unable to use. When it read by the connection, it will automatically
close the connection; therefore we lose the pointer which make me
have to use \textbf{skip} argument in it. Although using \textbf{skip}
for \textbf{read\_csv} will be faster than \textbf{read.csv}, its
speed is still very low.

<<function2, message=FALSE>>=
library(readr)
my.read_csv <- function(filename, n, total_row, maxrow=100000){   
		upper <- (total_row %/% maxrow) + 1   
		breaks <- seq(1, maxrow*upper + 1, by=maxrow)   
		col_name <- names(read_csv(filename, n_max=1))
		data <- matrix(0, nrow=n, ncol=length(col_name))   
		data <- as.data.frame(data)   
		#Avoid miss match   
		names(data) <- col_name   
		#Sample   
		n.row <- sample(1:total_row, n)   
		#Index   
		j <- 1   
		for(i in 1:(length(breaks) - 1)){     
			row_want <- which(n.row >= breaks[i] & n.row < breaks[i + 1])     
			if(length(row_want) == 0){
				next     
			}else{      
				data[j:(j + length(row_want) - 1), ] <- read_csv(filename, n_max=maxrow, skip=breaks[i] - 1)[n.row[row_want] - breaks[i] + 1, ]       
				j <- j + length(row_want)       
			}   
		}   
		data 
}

set.seed(1)
data <- my.read_csv("PS2data.gz", n=10000, total_row, maxrow=100000) 
head(data)
@

The next one is \textbf{readLines}. It read in each line as a character.
Hence, without any processing, I cannot just put the sample into the
all-0 data frame. Since the file is seperate by \textbf{,}. I can
use the function \textbf{strsplit} to split the character into different
columns by \textbf{,}. However, when I do this operation, some rows'
length is not 13 which is our total column number. The probelm present
as below. If there is a space next to the last comma, the \textbf{strsplit}
function will not give us that space. To addfress this problem, I
use \textbf{gsub} which replace all the \textbf{,} by \textbf{, }(with
a space in the end). Then using the \textbf{strsplit}, I can get the
last space, but another problem occurs. After spliting, the class
of the output is list. My strategy is to make the data into a numeric
matrix by the function, unlist and putting it into another matrix.
By doing so, I can input the value into our data frame.

<<example>>=
eg <- "I,am," 
strsplit(eg, split=",")
@

<<function3>>=
my.readLines <- function(filename, n, total_row, maxrow=100000){   
		upper <- (total_row %/% maxrow) + 1   
		breaks <- seq(1, maxrow*upper + 1, by=maxrow)   
		datacon <- file(filename, open="r")   
		col_name <- strsplit(readLines(datacon, n=1), split=",")[[1]]   
		data <- matrix(0, nrow=n, ncol=length(col_name))   
		data <- as.data.frame(data)   
		names(data) <- col_name  
		#Sample   
		n.row <- sample(1:total_row, n)   
		j <- 1   
		for(i in 1:(length(breaks) - 1)){     
			row_want <- which(n.row >= breaks[i] & n.row < breaks[i + 1])
			if(length(row_want) == 0){       
				next     
			}else{       
				data_row <- readLines(datacon, n=maxrow)[n.row[row_want] - breaks[i] + 1]       
				process <- as.numeric(unlist(strsplit(gsub(",", ", ", data_row), split=",")))       
				process_matrix <- matrix(process, nrow=length(row_want), ncol=length(col_name), byrow=TRUE)       
				data[j:(j + length(row_want) - 1), ] <- process_matrix   
				j <- j + length(row_want)        
			}   
		}   
		close(datacon)   
		data 
}

set.seed(1) 
data <- my.readLines("PS2data.gz", n=10000, total_row, maxrow=100000) 
head(data)
@

The last one is kind of a mix for above funtion(\textbf{readLines}
and \textbf{read.csv} or \textbf{read\_csv}). I notice that the \textbf{readLines}
with connection is really fast when reading data. I take advantage
of this point; however I do not make any processing in my for-loop.
Instead, I just retrieve those row I want by \textbf{readLines}. To
solve the problem, I can use the \textbf{writeLines} function to write
the output into csv file and use read\_csv to read the csv file. Therefore,
I can save some time for processing the output of \textbf{readLines}.

<<function4>>=
library(readr)
my.read.mix <- function(filename, n, total_row, maxrow=100000){   
		upper <- (total_row %/% maxrow) + 1   
		breaks <- seq(1, maxrow*upper + 1, by=maxrow)   
		datacon <- file(filename, open="r")   
		data <- matrix(0, nrow=n + 1)   
		data[1] <- readLines(datacon, n=1)   
		#Sample   
		n.row <- sample(1:total_row, n)   
		#Index   
		j <- 2   
		for(i in 1:(length(breaks) - 1)){     
			row_want <- which(n.row >= breaks[i] & n.row < breaks[i + 1])
			if(length(row_want) == 0){       
				next     
			}else{       
				data[j:(j + length(row_want) - 1)] <- readLines(datacon, n=maxrow)[n.row[row_want] - breaks[i] + 1]       
				j <- j + length(row_want)        
			}   
		}   
		close(datacon)   
		writeLines(data, "data.csv")   
		read_csv("data.csv") 
}

set.seed(1) 
data <- my.read.mix("PS2data.gz", n=10000, total_row, maxrow=100000)
head(data)
@

For overall speed performance, \textbf{readLines} is faster than \textbf{read.csv}
and \textbf{read.csv} is faster than \textbf{read\_csv}. The reason
why \textbf{readLines} is so fast is that it only read line by line
data withou processing and putting them into data.frame while the
\textbf{read.csv} does. I think why \textbf{read\_csv} is slow in
this situation is that when it read the compressed file, it will automatically
uncompress it. Also, I do not find a method to use connection with
\textbf{read\_csv} which forces me to use the argument skip. Therefore,
whenever it read the data in for-loop, it will uncompress the file.
I believe that it is the main reason why \textbf{read\_csv} cannot
outperform over \textbf{read.csv} in this case. Actually, I tried
to unzip the file and use the similar function(as \textbf{my.read\_csv})
to read it. Its performance may not be slower than \textbf{my.read.csv}.

<<comparison>>=
system.time(my.read.csv("PS2data.gz", n=10000, total_row, maxrow=100000)) 
@

<<comparison2, message=FALSE>>=
system.time(my.read_csv("PS2data.gz", n=10000, total_row, maxrow=100000))
@

<<comparison3>>=
system.time(my.readLines("PS2data.gz", n=10000, total_row, maxrow=100000))
@

<<comparison4>>=
system.time(my.read.mix("PS2data.gz", n=10000, total_row, maxrow=100000))
@

(c) I have done it first when I read in the data, which is that I
eliminated the column. The dim of processed data is \Sexpr{format(total_row, scientific=FALSE)}x13.
Compared to the original data, it reduced large size so I can read
it faster. 

(d) First of all, I make sure every column have the same value or
range as the description. Indeed, the range is correct. I will select
some variables to make some tables or graphs to detect wether there
is any interesting relationship between variable. 

<<range>>=
#Check the field 
summary(data)
@

<<correlation>>=
round(cor(data, use="complete.obs"), 2)
@

First I compare two variable FINCP(family incom) and TEN(tenure).
Originally, I think those family who have a higher income may not
have house mortgage which should belong to second category in TEN(own
free and clear). However, it turns out that overall family who owns
mortgage may have a higher income, although the difference may not
be statistically siginificant.

<<EDA1>>=
with(data, boxplot(FINCP ~ TEN, xlab="Tenure type", ylab="Income"))
@

Then I want to see the distribution of HHL(household language) among
ST(state code). It is quite hard to detect any pattern by table. Therefore,
I try to visualize them in a better way.

<<EDA2>>=
#Table
round(with(data, prop.table(table(HHL, ST), 2)), 2)
@

I used the ggplot to visualize the table by tile plot(geom\_tile).
We can find that there may be other second languages in household
for those state coded as 48, 35, 32, 15, 6. It turns out that \#48
is Texas, \#35 is New Mexico, \#32 is Nevado, \#15 is Hawaii and \#6
is California. Hawaii's Second language is other Indo-European languages
while other states are Spanish. 

<<EDA2b>>=
t <- as.data.frame(round(with(data, prop.table(table(HHL, ST), 2)), 2))
library(ggplot2) 
#Visualize 
ggplot(t) + geom_tile(aes(x=HHL, y=ST, z=Freq, fill=Freq)) 
@

Lastly, I make a table for NP(number of person records following the
house) and vehicle. I find that some family only have few person record
like 2, 3 or even 1, but they have 4 or more vehicles, which is quite
strange. Maybe they have different vehicle for diferent purpose. 

<<EDA3>>=
with(data, table(NP, VEH))
@

\end{document}
